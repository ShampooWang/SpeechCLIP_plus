data:
  dataset:
    name: flickr #flickr, coco
    dataset_root: /mnt/md1/user_jeffwang/Dataset/flickr
    text_file: Flickr8k.token.txt
    # alignment_file: Flickr_8k.ctm
    clip_image_transform: ViT-L/14
    load_image: true
    load_audio: true
    tokenizeText: true
    normalize_waveform: false
  batch_size: 128
  dev_batch_size: 6
  split_ratio: 0.9

model_settings:
  cascaded_objective_weight: 1.0
  parallel_objective_weight: 0.0
  keyword_objective_weight: 0.0
  matching_objective_weight: 0.0
  parallel_branch: 
    # name: integrate # [ "integrate", "individual" ]
    # integrate: add another [CLS] token for parallel objective
    # individual: add another [CLS] token and another branch transformer encoder for parallel objective
    transformer_args:
      type: TransformerEncoder
      n_layers: 1
      d_model: 1024
      nhead: 8
      dim_feedforward: 4096
      dropout: 0.1
      activation: gelu
      layer_norm_eps: 1.0e-5
      batch_first: true
      norm_first: false
  cascaded_branch:
    type: CascadedBranch_dynamic
    vq:
      activation: gelu # relu or gelu
      type: SimpleVectorQuantizer # which type of quantizer to use, gumbel or kmeans
      args:
          temp: fixed=0.1 #[2.0, 0.5, 0.999995]
          time_first: true
          use_gumbel: false
          hard: true
    downsampling:
      type: cif #cif, true_boundary
      cif:
        quantity_loss_weight: 0.25
        using_gt_len: false
        cif_output_dim: 1024
        encoder_embed_dim: 1024 # should be the innermost dimension of inputs
        produce_weight_type: "conv"
        cif_threshold: 1.0
        conv_cif_layer_num: 1
        conv_cif_width: 3 # 3 or 5
        conv_cif_output_channels_num: 1024
        conv_cif_dropout: 0.1
        dense_cif_units_num: 1024
        apply_scaling: true
        scaling_step: 5000
        apply_tail_handling: true
        tail_handling_firing_threshold: 0.5
        add_cif_ctxt_layers: false
    keyword:
      detokenized_K_neighbors: 5
      retrieve_method: cosine # cosine or pseudo_inverse
      batchnorms:
          type: eachKw #eachKw or same
          std_scale: 1.0
          learnable: true
          parallel: true
      kw_projection:
        dropout: 0.1
        dimensions: [1024, 1024, 768]
    transformer_args:
        type: MultiheadAttentionAndNorm
        n_layers: 1
        d_model: 1024
        nhead: 8
        dim_feedforward: 4096
        dropout: 0.1
        activation: gelu
        layer_norm_eps: 1.0e-5
        batch_first: true
        norm_first: false


cl_loss:
  type: MaskedContrastiveLoss # SupConLoss
  args:
    temperature: 0.07
    temperature_trainable: true
    margin: 0.0
    dcl: false
    a2b:  true
    b2a: true
# for SupConLoss
  # temperature: 1.0
  # base_temperature: 1.0
  # contrast_mode: all
  # learnable_temperature: true

retrieval:
  audio_feat_src: cascaded # ["parallel","cascaded"]
  recall_at: [1,5,10]


clip:
  name: ViT-L/14
  image_encoder_trainable: false
  text_encoder_trainable: false
  reduce_subword_embbedding: ./avssl/data/flickr_stat/text_clip_vocab_usage_byfreq.npy # coco_stat, flickr_stat

audio_encoder:
  type: FairseqHubert
  name: hubert_large_ll60k # wavlm_base wavlm_base_plus wavlm_large
  downsampling_rate: 320
  pretrained: true
  trainable: false
  feat_select_idx: weighted_sum # last_hidden_state, weighted_sum
  layer_drop: 0.0
  max_audio_len: 102400
  # reinit_layers: [10, 11]
  optim:
    name: Adam
    args:
      lr: 1.e-4
      weight_decay: 1.e-6
  scheduler:
    name: linear_warmup_decay
    warmup: 5000
    max_step: 50000
    final_lr: 1.e-8
    
trainer:
  max_steps: 50000
  gradient_clip_val: 4
  accumulate_grad_batches: 1
  check_val_every_n_epoch: 1
  precision: 16
  logger: wandb # wandb
  log_every_n_steps: 8
  num_sanity_val_steps: 0
  accelerator: gpu
  strategy: dp
  # fast_dev_run: 2
  # detect_anomaly: true
  # limit_train_batches: 2
  # limit_val_batches: 2

log_setting:
  log_detokenize_results: false # whether or not to save the results of detokenized vq output
  log_detokenize_results_every_n_epoch: 10
  log_draw_pca_every_n_epoch: 10 # 10

logger:
  project: kw-general